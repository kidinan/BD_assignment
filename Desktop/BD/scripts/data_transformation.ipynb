{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Necessary Libraries and Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, avg, stddev_samp, regexp_replace, to_date, count\n",
    "\n",
    "# Initialize Spark session with increased memory and optimized settings, running in local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaning\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.local.dir\", \"C:/temp/spark\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----------------+--------------------+-------------+----------------+--------+-----------+-------------+----------+-----------------+-----------+--------+--------------+---------------+--------+---------+---------------+\n",
      "|TransactionID|UserID|TransactionAmount|    TransactionDate1|PaymentMethod|MerchantCategory|Quantity|CustomerAge|     Location|DeviceType|TransactionStatus|Is_Declined|Is_Fraud|AccountAgeDays|TransactionDate|Latitude|Longitude|TransactionHour|\n",
      "+-------------+------+-----------------+--------------------+-------------+----------------+--------+-----------+-------------+----------+-----------------+-----------+--------+--------------+---------------+--------+---------+---------------+\n",
      "|            1| 29318|          1307.37|2023-12-19 19:47:...|       PayPal|   Entertainment|       5|         79|Oklahoma City|    Tablet|          Pending|          0|       0|            31|     2023-06-10| 35.4676| -97.5164|              8|\n",
      "|            2| 33379|         11643.05|2023-09-15 08:43:...|   Debit Card|     Electronics|       8|          7|      Chicago|    Mobile|        Completed|          0|       0|           128|     2023-09-16| 41.8781| -87.6298|             19|\n",
      "|            3| 25503|          2007.01|2023-07-26 13:48:...|       PayPal|     Electronics|       1|         70|    Nashville|   Desktop|        Completed|          0|       0|            15|     2023-04-14| 36.1627| -86.7816|              3|\n",
      "|            4|  7330|          4796.81|2023-12-30 18:32:...|  Credit Card|   Home & Garden|       5|         18|      Chicago|   Desktop|        Completed|          0|       0|           324|     2023-06-30| 41.8781| -87.6298|             19|\n",
      "|            5| 25851|           5992.9|2024-03-19 00:00:...|  Credit Card|        Clothing|       8|         12|       Austin|    Mobile|          Pending|          0|       0|           311|     2023-12-08| 30.2672| -97.7431|             19|\n",
      "+-------------+------+-----------------+--------------------+-------------+----------------+--------+-----------+-------------+----------+-----------------+-----------+--------+--------------+---------------+--------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specifying the dataset path\n",
    "file_path = 'C:/Users/a/Desktop/BD assignment/data/raw/ShopSpectra Transaction Dataset.csv'\n",
    "\n",
    "# Loading the dataset from the specified path\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Displaying the first few rows of the loaded dataset\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Remove rows with missing critical identifiers\n",
    "data = data.filter(F.col('UserID').isNotNull())\n",
    "data = data.filter(F.col('TransactionID').isNotNull())\n",
    "\n",
    "# Interpolate missing TransactionDate1 values\n",
    "window_spec = Window.partitionBy('UserID').orderBy('TransactionDate1')\n",
    "data = data.withColumn('TransactionDate1', F.last('TransactionDate1', ignorenulls=True).over(window_spec))\n",
    "\n",
    "# Fill missing numerical values with median\n",
    "numerical_cols = [col for col, dtype in data.dtypes if dtype in ['int', 'double']]\n",
    "numerical_cols.remove('UserID')  # Ensure UserID is not imputed\n",
    "\n",
    "for num_col in numerical_cols:\n",
    "    median_value = data.approxQuantile(num_col, [0.5], 0.01)[0]\n",
    "    data = data.fillna({num_col: median_value})\n",
    "\n",
    "# Fill missing categorical values with mode\n",
    "categorical_cols = [col for col, dtype in data.dtypes if dtype == 'string' and col != 'TransactionDate1']\n",
    "\n",
    "for cat_col in categorical_cols:\n",
    "    mode_value = data.groupBy(cat_col).count().orderBy('count', ascending=False).first()[0]\n",
    "    data = data.fillna({cat_col: mode_value})\n",
    "\n",
    "# Handle critical columns with domain knowledge\n",
    "data = data.withColumn('Is_Declined', F.when(F.col('Is_Declined').isNull(), 0).otherwise(F.col('Is_Declined')))\n",
    "data = data.withColumn('Is_Fraud', F.when(F.col('Is_Fraud').isNull(), 0).otherwise(F.col('Is_Fraud')))\n",
    "\n",
    "# Final cleanup: drop rows with remaining missing values in critical columns\n",
    "data = data.dropna(subset=['TransactionID', 'TransactionAmount', 'Quantity', 'CustomerAge', 'Location'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "data = data.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Correct Formatting Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize date formats\n",
    "date_cols = [col for col, dtype in data.dtypes if 'date' in col.lower()]\n",
    "\n",
    "for date_col in date_cols:\n",
    "    data = data.withColumn(date_col, to_date(col(date_col), 'yyyy-MM-dd'))\n",
    "# Clean numerical columns of non-numeric characters\n",
    "for num_col in numerical_cols:\n",
    "    data = data.withColumn(num_col, regexp_replace(col(num_col), '[^0-9.]', '').cast('double'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Column Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for consistency and clarity\n",
    "data = data.withColumnRenamed('TransactionID', 'transaction_id') \\\n",
    "           .withColumnRenamed('UserID', 'user_id') \\\n",
    "           .withColumnRenamed('TransactionAmount', 'transaction_amount') \\\n",
    "           .withColumnRenamed('TransactionDate1', 'transaction_date1') \\\n",
    "           .withColumnRenamed('PaymentMethod', 'payment_method') \\\n",
    "           .withColumnRenamed('MerchantCategory', 'merchant_category') \\\n",
    "           .withColumnRenamed('Quantity', 'quantity') \\\n",
    "           .withColumnRenamed('CustomerAge', 'customer_age') \\\n",
    "           .withColumnRenamed('Location', 'location') \\\n",
    "           .withColumnRenamed('DeviceType', 'device_type') \\\n",
    "           .withColumnRenamed('TransactionStatus', 'transaction_status') \\\n",
    "           .withColumnRenamed('Is_Declined', 'is_declined') \\\n",
    "           .withColumnRenamed('Is_Fraud', 'is_fraud') \\\n",
    "           .withColumnRenamed('AccountAgeDays', 'account_age_days') \\\n",
    "           .withColumnRenamed('Latitude', 'latitude') \\\n",
    "           .withColumnRenamed('Longitude', 'longitude') \\\n",
    "           .withColumnRenamed('TransactionHour', 'transaction_hour')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features\n",
    "user_transactions = data.groupBy(\"user_id\").agg(\n",
    "    count(\"transaction_id\").alias(\"total_transactions_per_user\"),\n",
    "    avg(\"transaction_amount\").alias(\"avg_transaction_amount_per_user\")\n",
    ")\n",
    "\n",
    "# Joining the new features with the original dataset\n",
    "data = data.join(user_transactions, on=\"user_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Save the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data successfully loaded and saved to C:/Users/a/Desktop/BD assignment/data/processed/clean_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "pandas_df = data.toPandas()\n",
    "\n",
    "# Specify the new path to save the cleaned data as a CSV file\n",
    "new_output_path = 'C:/Users/a/Desktop/BD assignment/data/processed/clean_data.csv'\n",
    "\n",
    "# Save the cleaned data as a CSV file for efficient storage and retrieval\n",
    "pandas_df.to_csv(new_output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data successfully loaded and saved to {new_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
